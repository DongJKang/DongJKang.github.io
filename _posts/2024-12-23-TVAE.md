---
title: "[Pytorch] Transformer-VAE 구현"
date: 2024-12-23 17:16:20 +0900
categories: [Pytorch]
tags: [vae, transformer]     # TAG names should always be lowercase
---

# 들어가기 앞서

본 포스트에서는 Transformer Encoder와 Decoder로 간단한 Variational Auto-encoder (VAE) 모델을 만들어 시계열 데이터를 생성해보려고 합니다.
구현에는 Pytorch를 사용합니다.

# 데이터

학습에 사용할 데이터는 *전기식 출입문 다변량 시계열 데이터*로 3개 변수를 가지고 있습니다. 훈련/검증 샘플 개수는 1,798/772개이고 모든 샘플의 길이는 109로 동일합니다.

아래는 데이터 예시로, 랜덤하게 뽑은 4개 샘플입니다.

![samples](../img/samples.png)

위 데이터가 특별히 의미있는 것은 아니고, 다만 Transformer-VAE 실습 예시로 적절할 것 같아 가져왔습니다.

데이터 세부사항은 이하 Dataset 구현 파트에서 다룹니다.

# 이론

구현을 위해서는 Transformer와 VAE에 대한 배경지식이 필요합니다. 이 두 가지 개념을 정말 친절하고 자세하게 설명하는 글이 이미 많기에 여기서는 생략하도록 하겠습니다.

아래는 제가 스터디할 때 가장 많은 도움을 받았던 링크입니다.

Transformer: [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)

VAE: [Variational Autoencoder - VISUALLY EXPLAINED!](https://youtu.be/h9kWaQQloPk)

# 구현

Transformer-VAE 모델을 구현하기에 앞서, 우선 필요한 함수들을 먼저 정의합니다.

## Dataset

```python
# 데이터 로딩 클래스 정의
class TransformerDataset(Dataset):
    def __init__(self, file_path, file_name, seq_len=None):
        """
        Args:
            file_path (string): 데이터 경로.
            file_name (string): 데이터 파일 이름.
            seq_len (int): 시퀀스 길이.
        """
        self.data = np.load(file_path + file_name, allow_pickle=True)
        if seq_len == None:
            seq_len = max([len(seq) for seq, _ in self.data])
        self.seq_len = seq_len
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pad_feature = np.ones((self.seq_len, 3)) * -0.1
        pad_feature[:len(self.data[idx][0])] = np.array(self.data[idx][0])
        
        return pad_feature
```

`torch.utils.data.Dataset`을 상속받아 `TransformerDataset`을 정의했습니다.

우선 padding으로 모든 샘플 길이를 맞춰준 것을 확인할 수 있습니다. Transformer는 RNN 계열으로 입력 시퀀스 길이를 반드시 맞춰주어야 하는 것은 아니지만, batch 연산을 위해 샘플 길이를 맞춰주었습니다. 그리고 보통 batch 연산을 위한 작업은 `torch.utils.data.DataLoader`에서 `collate_fn` 옵션으로 수행하는 것이 맞지만, 학습 데이터셋이 크지 않기에 코드 구현의 편의를 위해 `Dataset` 단계에서 통일해 주었습니다.   
(padding value는 -0.1로 설정하였으나, 이외 무엇으로 설정하든 이번 실습에서는 큰 차이가 없었습니다.)


## Positional Encoding

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first: bool = False):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.batch_first = batch_first

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # Shape [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]`` if batch_first=False
                             ``[batch_size, seq_len, embedding_dim]`` if batch_first=True
        """
        if self.batch_first:
            x = x + self.pe[:, :x.size(1)]
        else:
            x = x + self.pe[:, :x.size(0)].transpose(0, 1)
        return self.dropout(x)
```

Transformer 모델 input 단계에서 필요한 PositionalEncoding 모듈을 구현합니다. 익히 알려져 있듯, Transformer는 모든 시계열 데이터를 처음부터 끝까지 한번에 입력받기 때문에 각 데이터 포인트가 어느 시점에 해당하는지에 대한 정보를 따로 제공해주어야 합니다. PositionalEncoding 모듈은 각각의 시계열 데이터 포인트에 다른 값들을 더해줘서 해당 포인트가 어느 시점인지 모델이 알 수 있도록 합니다.

한가지 유의할 점은 `forward()` 부분의 `if self.batch_first` 구문입니다. 현재 `input.shape`가 `(seq_len, batch_size, embedding_dim)`이냐 `(batch_size, seq_len, embedding_dim)`이냐에 따라, 즉 내 입력 데이터 형식이 SBF냐 BSF이냐에 따라 출력 방식을 맞춰주어야 합니다. 그렇지 않으면 PositionalEncoding이 제대로 되지 않습니다.

* 정상적으로 구현된 케이스)   
`batch_first=False`일 때 데이터를 *SBF* 형태로 입력해주면
    ```python
    pos_encoder = PositionalEncoding(d_model=100, dropout=0, batch_first=False)
    a = torch.rand(1000, 1, 100) # SBF
    sns.heatmap(pos_encoder(a).squeeze())
    ```
![PE_right](../img/positional_encoding_right.png)

* 잘못 구현된 케이스)   
`batch_first=False`일 때 데이터를 *BSF* 형태로 입력해주면
    ```python
    pos_encoder = PositionalEncoding(d_model=100, dropout=0, batch_first=False)
    a = torch.rand(1, 1000, 100) # BSF
    sns.heatmap(pos_encoder(a).squeeze())
    ```
![PE_wrong](../img/positional_encoding_wrong.png)

## Transformer-VAE model

```python
class TVAE(torch.nn.Module):

    def __init__(self, n_feature, d_model, nhead, num_layers, hidden_dim):
        super().__init__()
        
        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=0.5, batch_first=True)

        self.seq_embedding = nn.Linear(n_feature, d_model)
        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=0.5, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)

        self.z_mean = nn.Linear(d_model, hidden_dim)
        self.z_log_var = nn.Linear(d_model, hidden_dim)
        self.z_embedding = nn.Linear(hidden_dim, d_model)
        
        decoder_layers = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=0.5, batch_first=True)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=num_layers)
        self.r_seq_embedding = nn.Linear(d_model, n_feature)
    
    def get_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def reparameterize(self, z_mean, z_log_var):
        eps = torch.randn_like(z_mean)
        z = z_mean + eps * torch.exp(z_log_var/2.) 
        return z
    
    def forward(self, src, tgt=None, src_key_padding_mask=None, tgt_key_padding_mask=None): # seq shape: (batch_size, seq_len, feature_dim)
        batch_size = src.shape[0]
        seq_len = src.shape[1]
        n_feature = src.shape[2]
        
        src = self.seq_embedding(src)
        src = self.pos_encoder(src)
        src = self.transformer_encoder(src, src_key_padding_mask)
        src = src.mean(1).squeeze()
        z_mean, z_log_var = self.z_mean(src), self.z_log_var(src)
        z = self.reparameterize(z_mean, z_log_var)
        src = self.z_embedding(z)
        src = src.unsqueeze(1)
        src = src.repeat(1, seq_len, 1)
        if self.training:
            tgt = tgt[:, :-1, :]
            tgt = torch.cat([torch.zeros(batch_size, 1, n_feature).float().to(DEVICE), tgt], dim=1)
            tgt = self.seq_embedding(tgt)
            tgt = self.pos_encoder(tgt)
            tgt_mask = self.get_mask(tgt.size(1)).to(DEVICE)

            tgt = self.transformer_decoder(
                tgt = tgt, 
                memory = src, 
                tgt_mask = tgt_mask, # to avoid looking at the future tokens (the ones on the right)
                tgt_key_padding_mask = tgt_key_padding_mask, # to avoid working on padding
                memory_key_padding_mask = src_key_padding_mask # avoid looking on padding of the src
            )

            recon_seq = self.r_seq_embedding(tgt)

        else:
            with torch.no_grad():
                recon_seq = torch.zeros(batch_size, 1, n_feature).float().to(DEVICE)
                
                for t in range(seq_len):
                    embed_recon_seq = self.seq_embedding(recon_seq)
                    embed_recon_seq = self.pos_encoder(embed_recon_seq)
                    recon_seq_mask = self.get_mask(recon_seq.size(1)).to(DEVICE)

                    output = self.transformer_decoder(
                        tgt=embed_recon_seq, 
                        memory=src, 
                        tgt_mask=recon_seq_mask,
                    )
                    output = output[:, -1, :]
                    output = self.r_seq_embedding(output)
                    output = output.unsqueeze(1)
                    recon_seq = torch.cat([recon_seq, output], dim=1)

                recon_seq = recon_seq[:, 1:, :]
                
        return recon_seq, z_mean, z_log_var
```

`torch.nn.Module`로 구현한 Transformer-VAE 모델입니다.

[TransformerEnccoder 구현에 참고한 링크](https://tutorials.pytorch.kr/beginner/transformer_tutorial.html)

[TransformerDecoder 구현에 참고한 링크](https://github.com/pytorch/tutorials/issues/719)

먼저 모델 선언 시 받는 인자는 5개 입니다.
```python
def __init__(self, n_feature, d_model, nhead, num_layers, hidden_dim):
```
`n_feature`: 입력 시퀀스 변수 개수   
`d_model`: Transformer의 d_model   
`nhead`: Transformer의 num_head   
`num_layer`: Transformer 레이어 개수   
`hidden_dim`: Latent vector 변수 개수

여기서 `d_model`, `nhead`, `num_layer`는 Transformer 모듈의 하이퍼파라미터이고, 우리가 잘 맞춰주어야 하는 것은 `n_feature`과 `hidden_dim`입니다. 본 실습의 경우 출입문 데이터의 변수가 3개이니 `n_feature=3`이고, `hidden_dim=10`으로 설정해주겠습니다.   
즉, `(109, 3)`의 입력 시퀀스를 10차원의 latent vector로 맵핑한 뒤, 다시 `(109, 3)`로 재구성하는 작업이 됩니다.

다음은 `forward()` 함수를 한줄씩 따라가 보며 모델 내부에서 어떤 일이 일어나는지 확인해보겠습니다.

첫 줄에서는 `forward()`가 받는 인자를 정의해줍니다. 인자는 총 4개 입니다. 

```python
def forward(self, src, tgt=None, src_key_padding_mask=None, tgt_key_padding_mask=None): # seq shape: (batch_size, seq_len, feature_dim)
```

모델은 입력 시퀀스 `src`와 출력 시퀀스 `tgt`을 받습니다. 해당 모델은 *teacher forcing*을 사용하여 학습되므로 학습 시 label에 해당하는 `tgt` 시퀀스를 입력해줘야 합니다. 본 실습에서 입력 시퀀스의 형태는 BSF 즉, `(batch_size, seq_len, feature_dim)`로 가정합니다.

그리고 `src_key_padding_mask`와 `tgt_key_padding_mask`를 받습니다. 이것은 모델 학습 시, 학습과 관련이 없는 데이터 포인트를 무시하기 위해 사용하는 mask입니다. 위에서 `Dataset`을 구현할 때 단지 샘플 길이를 맞추기 위한 용도로 각 샘플의 맨 뒤에 padding value -0.1을 붙였습니다. 이러한 값은 사실 실질적인 의미를 가지지 않기에 무시해야 합니다. 이때 padding의 위치를 나타내는 mask 행렬(padding이면 1, 아니면 0)을 입력해주면 Transformer 모델이 해당 포인트에 대해서는 유사도를 구하지 않습니다. (본 실습에서는 padding mask를 따로 사용하지 않습니다. 즉, `src_key_padding_mask=None`, `tgt_key_padding_mask=None`)

그 다음은 입력 시퀀스의 형태에 따라 `batch_size`, `seq_len`, `n_feature` 변수를 정의하고, latent vector `z`를 계산합니다.
```python
batch_size = src.shape[0]
seq_len = src.shape[1]
n_feature = src.shape[2]

src = self.seq_embedding(src)
src = self.pos_encoder(src)
src = self.transformer_encoder(src, src_key_padding_mask)
src = src.mean(1).squeeze()
z_mean, z_log_var = self.z_mean(src), self.z_log_var(src)
z = self.reparameterize(z_mean, z_log_var)
```

계산 순서는 이렇습니다.   
1. Embedding: `(batch_size, seq_len, n_feature)` -> `(batch_size, seq_len, d_model)`   
입력 시퀀스는 변수 개수가 3이라 TransformerEncoder 입력으로 사용하기엔 차원이 너무 작습니다. 따라서 차원 개수를 늘려줍니다.

2. PositionalEncoding: `(batch_size, seq_len, d_model)` -> `(batch_size, seq_len, d_model)`   
PositionalEncoding 값을 더해줍니다.

3. TransformerEncoder: `(batch_size, seq_len, d_model)` -> `(batch_size, seq_len, d_model)`   
TransformerEncoder 모듈 내부에서 연산을 수행합니다.

4. Vetorizing: `(batch_size, seq_len, d_model)` -> `(batch_size, d_model)`   
Transformer 모듈의 출력 형태는 입력과 동일한 시퀀스이기 때문에 이후 latent vector를 얻기 위해 벡터화 과정을 따로 수행해줍니다.
방법에는 여러가지가 있는데, 여기서는 sequence_dim으로 mean을 취해주는 방식을 사용했습니다.

    방법 1: [sequence_dim으로 mean or sum 적용](https://stackoverflow.com/questions/65190217/how-to-process-transformerencoderlayer-output-in-pytorch)   
    (간단히 sequence 차원으로 평균 or 합계를 적용해주는 방식.)

    방법 2: [sequence_dim으로 large stride conv layer 적용](https://discuss.pytorch.org/t/transformers-to-encode-a-sequence-into-a-fixed-lenght-vector/102290)   
    (일정 time-step 시퀀스를 Conv1d (stride>5)로 tokenize하는 방식. Token input에 강한 Transformer 특성과 잘 맞을 것이라 생각.)

5. Reparameterizing: `(batch_size, d_model)` -> `(batch_size, hidden_dim)`   
그리고 이 벡터를 `self.z_mean()`과 `self.z_log_var` layer에 넣어 `z_mean`과 `z_log_var`를 구하고 *reparameterization*으로 latent vector `z`를 구합니다.


이제는 주어진 latent vector `z`로 입력 시퀀스를 재구성할 차례입니다.   
우선 TransformerDecoder의 입력 형식에 맞도록 `z`를 다시 시퀀스 형태로 변환해줍니다.

```python
src = self.z_embedding(z) # (batch_size, hidden_dim) -> (batch_size, d_model)
src = src.unsqueeze(1) # (batch_size, d_model) -> (batch_size, 1, d_model)
src = src.repeat(1, seq_len, 1) # (batch_size, 1, d_model) -> (batch_size, seq_len, d_model)
```

이후는 학습과 추론 두 가지 형태로 나뉩니다.

참고) `torch.nn.Module`로 구현한 클래스 내부에서 `if self.training:` 구문을 사용하면 `model.train()`인지 `model.eval()`인지를 알 수 있습니다.

학습일 떄

```python
if self.training:
    tgt = tgt[:, :-1, :]
    tgt = torch.cat([torch.zeros(batch_size, 1, n_feature).float().to(DEVICE), tgt], dim=1)
    tgt = self.seq_embedding(tgt)
    tgt = self.pos_encoder(tgt)
    tgt_mask = self.get_mask(tgt.size(1)).to(DEVICE)

    tgt = self.transformer_decoder(
        tgt = tgt, 
        memory = src, 
        tgt_mask = tgt_mask, # to avoid looking at the future tokens (the ones on the right)
        tgt_key_padding_mask = tgt_key_padding_mask, # to avoid working on padding
        memory_key_padding_mask = src_key_padding_mask # avoid looking on padding of the src
    )

    recon_seq = self.r_seq_embedding(tgt)
```

*teacher_forcing*을 위한 `tgt`을 먼저 준비해줍니다. 여기서 `tgt`의 첫 번째 시점은 항상 0 벡터여야 하므로, `tgt`의 마지막 시점 벡터를 지우고 제일 앞에 0 벡터를 붙여줍니다. (NLP의 `<sos>` 토큰과 같습니다.)   
그 이후의 과정은 Encoding 때와 동일하지만 한가지, `tgt_mask`가 추가되었습니다. `self.get_mask()`로 `tgt_mask`를 생성하고 TransformerDecoder 선언 시 인자로 넣어줍니다. 이 `tgt_mask`는 *look ahead mask*라고도 하며, Decoder 추론 시 미래 시점의 데이터를 모델이 참고하지 못하도록 막아주는 역할을 합니다.

입력 시퀀스 길이가 5일 경우 위 코드를 실행하면
```
model.get_mask(5)
tensor([[0., -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0.]])
```
위와 같이 대각 성분이 0인 상삼각행렬을 출력합니다. 그러면 `tgt_mask` 적용 시 현재 추론 시점보다 미래 시점의 데이터는 모두 내부 연산과정에서 0이 됩니다.

그리고 마지막 `self.r_seq_embedding()`으로 입력 시퀀스와 동일한 형태의 결과값을 얻습니다.

추론일 때

```python
else:
    with torch.no_grad():
        recon_seq = torch.zeros(batch_size, 1, n_feature).float().to(DEVICE)
        
        for t in range(seq_len):
            embed_recon_seq = self.seq_embedding(recon_seq)
            embed_recon_seq = self.pos_encoder(embed_recon_seq)
            recon_seq_mask = self.get_mask(recon_seq.size(1)).to(DEVICE)

            output = self.transformer_decoder(
                tgt=embed_recon_seq, 
                memory=src, 
                tgt_mask=recon_seq_mask,
            )
            output = output[:, -1, :]
            output = self.r_seq_embedding(output)
            output = output.unsqueeze(1)
            recon_seq = torch.cat([recon_seq, output], dim=1)

        recon_seq = recon_seq[:, 1:, :]
```

제일 먼저 `with torch.no_grad():`를 선언하고 TransformerDecoder를 활용한 inference 구문을 그 내부에서 작성합니다. 그렇지 않으면 추론시 메모리가 뻗어버리는 문제가 발생합니다.

추론할 때는 *teacher forcing*을 사용하지 않고, 대신 for문을 사용하여 결과값을 순차적으로 생성하게끔 만들어줍니다.   
학습할 때와 마찬가지로 첫 시점의 입력은 0 벡터를 사용합니다. 그리고 최종 출력 시에 0 벡터는 제외해줍니다.   
그 외는 학습할 때와 동일합니다.

최종적으로 `forward()`함수는 재구성된 입력 시퀀스, KL 정규화 loss를 위한 z_mean, z_log_var를 반환합니다.
```python
return recon_seq, z_mean, z_log_var
```

# 학습

이제 학습해보도록 하겠습니다.

```
testbed_dataset = TransformerDataset(file_path=data_path, file_name='testbed_multiclass_dataset.npy')

sample_size = len(testbed_dataset)
train_size = int(sample_size * 0.7)
test_size = sample_size - train_size
train_dataset, test_dataset = random_split(testbed_dataset, [train_size, test_size])
```

```
len(train_dataset), len(test_dataset)
>>> (1798, 772)
```


```
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
```

```
# 데이터셋 배치 수
print(f'#train batch: {len(train_dataloader)}')
print(f'#vali batch: {len(test_dataloader)}')
>>> #train batch: 14
    #vali batch: 6
```

```
# 입력 데이터 크기
# 리스트 첫번째 값: 배치 이미지
# 크기: 배치사이즈, 시퀀스길이(100), 변수(3)
# 리스트 두번째 값: 배치 라벨
features = next(iter(train_dataloader))
print(f'train: {features.shape}')
features = next(iter(test_dataloader))
print(f'vali: {features.shape}')
>>> train: torch.Size([128, 109, 3])
    vali: torch.Size([128, 109, 3])
```

```
model = TVAE(
    n_feature=3,
    d_model=128,
    nhead=8,
    num_layers=3,
    hidden_dim=10
)

model = model.to(DEVICE)
```

```
optim = torch.optim.Adam(model.parameters(),
                             lr=LEARNING_RATE)
recon_loss_fn = nn.MSELoss()
```

```
def kl_loss_fn(z_mean, z_log_var):
    kl_loss = -0.5 * torch.sum(1 + z_log_var 
                                      - z_mean**2 
                                      - torch.exp(z_log_var), 
                                      axis=1) # sum over latent dimension
    kl_loss = kl_loss.mean() # average over batch dimension
    return kl_loss
```

