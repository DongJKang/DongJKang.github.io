---
title: "[Pytorch] Transformer-VAE 구현"
date: 2024-12-23 17:16:20 +0900
categories: [Pytorch]
tags: [vae, transformer]     # TAG names should always be lowercase
---

# 들어가기 앞서

본 포스트에서는 Transformer Encoder와 Decoder로 간단한 Variational Auto-encoder (VAE) 모델을 만들어 time-series 데이터를 생성해보려고 합니다.   
현존하는 Transformer 모델 구현 예제들이 대부분 NLP 문제만을 다루고 있어, 스터디 당시 time-series 데이터의 경우에는 어떻게 해야 하는지에 대한 어려움이 있었습니다.   
그래서 포스팅의 목적은 "time-series 데이터를 위한 Transformer 모델 구현 방법"에 있고, TransformerEncoder와 Decoder를 모두 다루기 위해 Transformer-VAE를 구현합니다.   
본문에는 구현 시 참고할 만한 요소들을 최대한 담고자 하였습니다.   
실습은 Jupyter Notebook에서 수행합니다.

# 데이터

학습에 사용할 데이터는 *전기식 출입문 다변량 시계열 데이터*로 3개 변수를 가지고 있습니다.
훈련/검증 샘플 개수는 1,798/772개이고 모든 샘플의 길이는 109로 동일합니다.

아래는 데이터 예시로, 랜덤하게 뽑은 4개 샘플입니다.

![samples](../img/samples.png)

위 데이터를 재구성하는 것에 특별한 의미가 있지는 않고, 다만 time-series Transformer-VAE 실습에 적절한 예시인 것 같아 가져왔습니다.

데이터 세부사항은 이하 Dataset 구현 파트에서 다룹니다.

# 이론

구현을 위해서는 Transformer와 VAE에 대한 배경지식이 필요합니다.
이 두 가지 개념을 정말 친절하고 자세하게 설명하는 글이 이미 많기에 여기서는 생략하도록 하겠습니다.

아래는 제가 스터디할 때 가장 많은 도움을 받았던 링크입니다.

Transformer: [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/31379)

VAE: [Variational Autoencoder - VISUALLY EXPLAINED! (Youtube 영상)](https://youtu.be/h9kWaQQloPk)

# 구현

Transformer-VAE 모델을 구현하기에 앞서, 우선 필요한 함수들을 먼저 정의합니다.

## Dataset

```python
# 데이터 로딩 클래스 정의
class TransformerDataset(Dataset):
    def __init__(self, file_path, file_name, seq_len=None):
        """
        Args:
            file_path (string): 데이터 경로.
            file_name (string): 데이터 파일 이름.
            seq_len (int): 시퀀스 길이.
        """
        self.data = np.load(file_path + file_name, allow_pickle=True)
        if seq_len == None:
            seq_len = max([len(seq) for seq, _ in self.data])
        self.seq_len = seq_len
    
    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pad_feature = np.ones((self.seq_len, 3)) * -0.1
        pad_feature[:len(self.data[idx][0])] = np.array(self.data[idx][0])
        
        return pad_feature
```

`torch.utils.data.Dataset`을 상속받아 `TransformerDataset`을 정의했습니다.

Transformer는 RNN 계열으로 `Dataset`의 입력 시퀀스 길이를 맞춰주지 않아도 됩니다.
다만 batch 연산을 위해 각 batch 단위로는 시퀀스 길이를 맞춰줘야 하는데요, 이 때는 보통 `torch.utils.data.DataLoader`에서 `collate_fn` 옵션을 사용합니다.

하지만 이번 실습에서는 학습 데이터셋이 크지 않기에 코드 구현의 편의를 위해 `Dataset` 단계에서 통일해 주었습니다.
그래서 padding으로 모든 샘플 길이를 맞춰준 것을 확인할 수 있습니다. (padding value는 -0.1로 설정하였으나, 이외 무엇으로 설정하든 이번 실습에서는 결과에 큰 차이가 없었습니다.)


## Positional Encoding

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first: bool = False):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.batch_first = batch_first

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # Shape [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]`` if batch_first=False
                             ``[batch_size, seq_len, embedding_dim]`` if batch_first=True
        """
        if self.batch_first:
            x = x + self.pe[:, :x.size(1)]
        else:
            x = x + self.pe[:, :x.size(0)].transpose(0, 1)
        return self.dropout(x)
```

Transformer 모델 input 단계에서 필요한 PositionalEncoding 모듈을 구현합니다.
Transformer는 모든 시계열 데이터를 처음부터 끝까지 한번에 입력받기 때문에 각 데이터 포인트가 어느 시점에 해당하는지에 대한 정보를 따로 제공해주어야 합니다.
PositionalEncoding 모듈은 각각의 시계열 데이터 포인트에 다른 값들을 더해줘서 해당 포인트가 어느 시점인지 모델이 알 수 있도록 합니다.

한가지 유의할 점은 `forward()` 부분의 `if self.batch_first` 구문입니다.
현재 `input.shape`가 `(seq_len, batch_size, embedding_dim)`이냐 `(batch_size, seq_len, embedding_dim)`이냐에 따라, 즉 내 입력 데이터 형식이 SBF냐 BSF이냐에 따라 출력 방식을 맞춰주어야 합니다.
그렇지 않으면 PositionalEncoding이 제대로 되지 않습니다.

* 정상적으로 구현된 케이스)   
`batch_first=False`일 때 데이터를 *SBF* 형태로 입력해주면
    ```python
    pos_encoder = PositionalEncoding(d_model=100, dropout=0, batch_first=False)
    a = torch.rand(1000, 1, 100) # SBF
    sns.heatmap(pos_encoder(a).squeeze())
    ```
![PE_right](../img/positional_encoding_right.png)

* 잘못 구현된 케이스)   
`batch_first=False`일 때 데이터를 *BSF* 형태로 입력해주면
    ```python
    pos_encoder = PositionalEncoding(d_model=100, dropout=0, batch_first=False)
    a = torch.rand(1, 1000, 100) # BSF
    sns.heatmap(pos_encoder(a).squeeze())
    ```
![PE_wrong](../img/positional_encoding_wrong.png)

## Transformer-VAE model

```python
class TVAE(torch.nn.Module):

    def __init__(self, n_feature, d_model, nhead, num_layers, hidden_dim):
        super().__init__()
        
        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=0.5, batch_first=True)

        self.seq_embedding = nn.Linear(n_feature, d_model)
        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=0.5, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)

        self.z_mean = nn.Linear(d_model, hidden_dim)
        self.z_log_var = nn.Linear(d_model, hidden_dim)
        self.z_embedding = nn.Linear(hidden_dim, d_model)
        
        decoder_layers = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=0.5, batch_first=True)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_layers=num_layers)
        self.r_seq_embedding = nn.Linear(d_model, n_feature)
    
    def get_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def reparameterize(self, z_mean, z_log_var):
        eps = torch.randn_like(z_mean)
        z = z_mean + eps * torch.exp(z_log_var/2.) 
        return z
    
    def forward(self, src, tgt=None, src_key_padding_mask=None, tgt_key_padding_mask=None): # seq shape: (batch_size, seq_len, feature_dim)
        batch_size = src.shape[0]
        seq_len = src.shape[1]
        n_feature = src.shape[2]
        
        src = self.seq_embedding(src)
        src = self.pos_encoder(src)
        src = self.transformer_encoder(src, src_key_padding_mask)
        src = src.mean(1).squeeze()
        z_mean, z_log_var = self.z_mean(src), self.z_log_var(src)
        z = self.reparameterize(z_mean, z_log_var)
        src = self.z_embedding(z)
        src = src.unsqueeze(1)
        src = src.repeat(1, seq_len, 1)
        if self.training:
            tgt = tgt[:, :-1, :]
            tgt = torch.cat([torch.zeros(batch_size, 1, n_feature).float().to(DEVICE), tgt], dim=1)
            tgt = self.seq_embedding(tgt)
            tgt = self.pos_encoder(tgt)
            tgt_mask = self.get_mask(tgt.size(1)).to(DEVICE)

            tgt = self.transformer_decoder(
                tgt = tgt, 
                memory = src, 
                tgt_mask = tgt_mask, # to avoid looking at the future tokens (the ones on the right)
                tgt_key_padding_mask = tgt_key_padding_mask, # to avoid working on padding
                memory_key_padding_mask = src_key_padding_mask # avoid looking on padding of the src
            )

            recon_seq = self.r_seq_embedding(tgt)

        else:
            with torch.no_grad():
                recon_seq = torch.zeros(batch_size, 1, n_feature).float().to(DEVICE)
                
                for t in range(seq_len):
                    embed_recon_seq = self.seq_embedding(recon_seq)
                    embed_recon_seq = self.pos_encoder(embed_recon_seq)
                    recon_seq_mask = self.get_mask(recon_seq.size(1)).to(DEVICE)

                    output = self.transformer_decoder(
                        tgt=embed_recon_seq, 
                        memory=src, 
                        tgt_mask=recon_seq_mask,
                    )
                    output = output[:, -1, :]
                    output = self.r_seq_embedding(output)
                    output = output.unsqueeze(1)
                    recon_seq = torch.cat([recon_seq, output], dim=1)

                recon_seq = recon_seq[:, 1:, :]
                
        return recon_seq, z_mean, z_log_var
```

`torch.nn.Module`로 구현한 Transformer-VAE 모델입니다.

[TransformerEnccoder 구현에 참고한 링크 - Pytorch 공식문서 튜토리얼](https://tutorials.pytorch.kr/beginner/transformer_tutorial.html)   
[TransformerDecoder 구현에 참고한 링크 - Pytorch 공식 Github](https://github.com/pytorch/tutorials/issues/719)

먼저 TVAE 모델 선언 시 받는 인자는 5개 입니다.
```python
def __init__(self, n_feature, d_model, nhead, num_layers, hidden_dim):
```
`n_feature`: 입력 시퀀스 변수 개수   
`d_model`: Transformer의 d_model   
`nhead`: Transformer의 num_head   
`num_layer`: Transformer 레이어 개수   
`hidden_dim`: Latent vector 변수 개수

여기서 `d_model`, `nhead`, `num_layer`는 Transformer 모듈의 하이퍼파라미터이고, 우리가 잘 맞춰주어야 하는 것은 `n_feature`과 `hidden_dim`입니다.
본 실습의 경우 출입문 데이터의 변수가 3개이니 `n_feature=3`이고, `hidden_dim=2`으로 설정해주겠습니다.   
즉, `(109, 3)`의 입력 시퀀스를 `(2)`의 latent vector(정규분포에서 샘플링되는)로 맵핑한 뒤, 다시 `(109, 3)`로 재구성하는 작업이 됩니다.

다음은 `forward()` 함수를 한줄씩 따라가 보며 모델 내부에서 어떤 일이 일어나는지 확인해보겠습니다.

첫 줄에서는 `forward()`가 받는 인자를 정의해줍니다. 인자는 총 4개 입니다. 

```python
def forward(self, src, tgt=None, src_key_padding_mask=None, tgt_key_padding_mask=None): # seq shape: (batch_size, seq_len, feature_dim)
```

모델은 입력 시퀀스 `src`와 출력 시퀀스 `tgt`을 받습니다.
해당 모델은 *teacher forcing*을 사용하여 학습되므로 학습 시 label에 해당하는 `tgt` 시퀀스를 입력해줘야 합니다.
본 실습에서 입력 시퀀스의 형태는 BSF 즉, `(batch_size, seq_len, feature_dim)`로 가정합니다.

그리고 `src_key_padding_mask`와 `tgt_key_padding_mask`를 받습니다.
이것은 모델 학습 시, 학습과 관련이 없는 데이터 포인트를 무시하기 위해 사용하는 mask입니다.
위에서 `Dataset`을 구현할 때 단지 샘플 길이를 맞추기 위한 용도로 각 샘플의 맨 뒤에 padding value -0.1을 붙였습니다.
이러한 값은 사실 실질적인 의미를 가지지 않기에 무시해야 합니다.
이때 padding의 위치를 나타내는 mask 행렬(padding이면 1, 아니면 0)을 입력해주면 Transformer 모델이 해당 포인트에 대해서는 유사도를 구하지 않습니다.
(본 실습에서는 padding mask를 따로 사용하지 않습니다. 즉, `src_key_padding_mask=None`, `tgt_key_padding_mask=None`)

그 다음은 입력 시퀀스의 형태에 따라 `batch_size`, `seq_len`, `n_feature` 변수를 정의하고, latent vector `z`를 계산합니다.
```python
batch_size = src.shape[0]
seq_len = src.shape[1]
n_feature = src.shape[2]

src = self.seq_embedding(src)
src = self.pos_encoder(src)
src = self.transformer_encoder(src, src_key_padding_mask)
src = src.mean(1).squeeze()
z_mean, z_log_var = self.z_mean(src), self.z_log_var(src)
z = self.reparameterize(z_mean, z_log_var)
```

계산 순서는 이렇습니다.   
1. Embedding: `(batch_size, seq_len, n_feature)` -> `(batch_size, seq_len, d_model)`   
NLP의 token embedding 단계와 같습니다. 입력 시퀀스를 더 높은 차원으로 tokenize해줍니다. 간단하게 Fully Connected Layer를 사용합니다.

2. PositionalEncoding: `(batch_size, seq_len, d_model)` -> `(batch_size, seq_len, d_model)`   
PositionalEncoding 값을 더해줍니다.

3. TransformerEncoder: `(batch_size, seq_len, d_model)` -> `(batch_size, seq_len, d_model)`   
TransformerEncoder 모듈 내부에서 연산을 수행합니다.

4. Vetorizing: `(batch_size, seq_len, d_model)` -> `(batch_size, d_model)`   
Transformer 모듈의 출력 형태는 입력과 동일한 시퀀스이기 때문에 이후 latent vector를 얻기 위해 벡터화 과정을 따로 수행해줍니다.
방법에는 여러가지가 있는데, 여기서는 sequence_dim으로 mean을 취해주는 방식을 사용합니다.

    방법 1: [sequence_dim으로 mean or sum 적용](https://stackoverflow.com/questions/65190217/how-to-process-transformerencoderlayer-output-in-pytorch)   
    (간단히 sequence 차원으로 평균 or 합계를 적용해주는 방식.)

    방법 2: [sequence_dim으로 large stride conv layer 적용](https://discuss.pytorch.org/t/transformers-to-encode-a-sequence-into-a-fixed-lenght-vector/102290)   
    (일정 time-step 시퀀스를 Conv1d (stride>5)로 tokenize하는 방식. Token input에 강한 Transformer 특성과 잘 맞을 것이라 생각.)

5. Reparameterizing: `(batch_size, d_model)` -> `(batch_size, hidden_dim)`   
그리고 이 벡터를 `self.z_mean()`과 `self.z_log_var` layer에 넣어 `z_mean`과 `z_log_var`를 구하고 *reparameterization*으로 latent vector `z`를 구합니다.


이제는 주어진 latent vector `z`로 입력 시퀀스를 재구성할 차례입니다.   
우선 TransformerDecoder의 입력 형식에 맞도록 `z`를 다시 시퀀스 형태로 변환해줍니다.

```python
src = self.z_embedding(z) # (batch_size, hidden_dim) -> (batch_size, d_model)
src = src.unsqueeze(1) # (batch_size, d_model) -> (batch_size, 1, d_model)
src = src.repeat(1, seq_len, 1) # (batch_size, 1, d_model) -> (batch_size, seq_len, d_model)
```

이후는 학습과 추론 두 가지 형태로 나뉩니다.

참고) `torch.nn.Module`로 구현한 `model` 내부에서 `if self.training:` 구문을 사용하면 `model.train()`인지 `model.eval()`인지를 알 수 있습니다.

학습일 떄

```python
if self.training:
    tgt = tgt[:, :-1, :]
    tgt = torch.cat([torch.zeros(batch_size, 1, n_feature).float().to(DEVICE), tgt], dim=1)
    tgt = self.seq_embedding(tgt)
    tgt = self.pos_encoder(tgt)
    tgt_mask = self.get_mask(tgt.size(1)).to(DEVICE)

    tgt = self.transformer_decoder(
        tgt = tgt, 
        memory = src, 
        tgt_mask = tgt_mask, # to avoid looking at the future tokens (the ones on the right)
        tgt_key_padding_mask = tgt_key_padding_mask, # to avoid working on padding
        memory_key_padding_mask = src_key_padding_mask # avoid looking on padding of the src
    )

    recon_seq = self.r_seq_embedding(tgt)
```

*teacher_forcing*을 위한 `tgt`을 먼저 준비해줍니다.
여기서 `tgt`의 첫 번째 시점은 항상 0 벡터여야 하므로, `tgt`의 마지막 시점 벡터를 지우고 제일 앞에 0 벡터를 붙여줍니다.
(0 벡터는 NLP의 `<sos>` 토큰과 같습니다.)   
그 이후의 과정은 Encoding 때와 동일하지만 한가지, `tgt_mask`가 추가되었습니다.
`self.get_mask()`로 `tgt_mask`를 생성하고 TransformerDecoder 선언 시 인자로 넣어줍니다.
이 `tgt_mask`는 *look ahead mask*라고도 하며, Decoder 추론 시 미래 시점의 데이터를 모델이 참고하지 못하도록 막아주는 역할을 합니다.

가령 시퀀스 길이를 5로 설정하고 마스크 행렬을 만들면,
```
model.get_mask(5)
tensor([[0., -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., -inf],
        [0., 0., 0., 0., 0.]])
```
위와 같이 대각 성분이 0인 상삼각행렬을 출력합니다.
행렬의 우상 부분은 모두 음의 무한대 값을 가지므로 `tgt_mask` 적용 시 현재 추론 시점보다 미래 시점의 데이터는 모두 내부 연산과정에서 0이 됩니다.

그리고 마지막 `self.r_seq_embedding()`으로 입력 시퀀스와 동일한 형태의 결과값을 얻습니다.

추론일 때

```python
else:
    with torch.no_grad():
        recon_seq = torch.zeros(batch_size, 1, n_feature).float().to(DEVICE)
        
        for t in range(seq_len):
            embed_recon_seq = self.seq_embedding(recon_seq)
            embed_recon_seq = self.pos_encoder(embed_recon_seq)
            recon_seq_mask = self.get_mask(recon_seq.size(1)).to(DEVICE)

            output = self.transformer_decoder(
                tgt=embed_recon_seq, 
                memory=src, 
                tgt_mask=recon_seq_mask,
            )
            output = output[:, -1, :]
            output = self.r_seq_embedding(output)
            output = output.unsqueeze(1)
            recon_seq = torch.cat([recon_seq, output], dim=1)

        recon_seq = recon_seq[:, 1:, :]
```

제일 먼저 `with torch.no_grad():`를 선언하고 TransformerDecoder를 활용한 inference 구문을 그 내부에서 작성합니다.
그렇지 않으면 추론시 메모리가 뻗어버리는 문제가 발생합니다.

추론할 때는 *teacher forcing*을 사용하지 않고, 대신 for문을 사용하여 결과값을 순차적으로 생성하게끔 만들어줍니다.
학습할 때와 마찬가지로 첫 시점의 입력은 0 벡터를 사용합니다.
그리고 최종 출력 시에 0 벡터는 제외해줍니다.
그 외는 학습할 때와 동일합니다.

최종적으로 `forward()`함수는 재구성된 입력 시퀀스 `recon_seq`, KL 정규화 loss를 위한 `z_mean`, `z_log_var`를 반환합니다.
```python
return recon_seq, z_mean, z_log_var
```

# 학습

필요한 모듈들을 모두 선언해주었으니, 이제 학습을 시작합니다.

먼저 훈련/검증 데이터셋을 준비합니다. 비율은 0.7:0.3으로 하겠습니다.
```python
testbed_dataset = TransformerDataset(file_path=data_path, file_name='testbed_multiclass_dataset.npy')

sample_size = len(testbed_dataset)
train_size = int(sample_size * 0.7)
test_size = sample_size - train_size
train_dataset, test_dataset = random_split(testbed_dataset, [train_size, test_size])

len(train_dataset), len(test_dataset)
>>> (1798, 772)
```

그리고 학습/테스트 데이터로더를 준비합니다. `BATCH_SIZE=128`으로 설정했습니다.
```
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)

# 데이터셋 배치 수
print(f'#train batch: {len(train_dataloader)}')
print(f'#vali batch: {len(test_dataloader)}')
>>> #train batch: 14
    #vali batch: 6
```

데이터로더로 생성한 입력 데이터의 크기가 의도한대로 출력되는지 확인합니다.
```
# 입력 데이터 크기
features = next(iter(train_dataloader))
print(f'train: {features.shape}')
features = next(iter(test_dataloader))
print(f'vali: {features.shape}')
>>> train: torch.Size([128, 109, 3])
    vali: torch.Size([128, 109, 3])
```

이제 모델을 선언합니다.
`n_feature=3`, `hidden_dim=2`로 주었고, 나머지는 임의로 설정했습니다.
```
model = TVAE(
    n_feature=3,
    d_model=128,
    nhead=8,
    num_layers=3,
    hidden_dim=2,
)

model = model.to(DEVICE)
```

Optimizer로는 Adam을 사용하고 `LEARNING_RATE=0.0001`로 설정합니다.
```
optim = torch.optim.Adam(model.parameters(),
                             lr=LEARNING_RATE)
```

다음은 손실함수입니다. VAE의 손실함수인 reconstruction_loss와 kl_loss를 선언해줍니다.
```
recon_loss_fn = nn.MSELoss()
```

```
def kl_loss_fn(z_mean, z_log_var):
    kl_loss = -0.5 * torch.sum(1 + z_log_var 
                                      - z_mean**2 
                                      - torch.exp(z_log_var), 
                                      axis=1) # sum over latent dimension
    kl_loss = kl_loss.mean() # average over batch dimension
    return kl_loss
```

이제 학습을 시작합니다.
```python
train_epoch_loss_list = []
train_epoch_recon_loss_list = []
train_epoch_kl_loss_list = []
val_epoch_loss_list = []
val_epoch_recon_loss_list = []
val_epoch_kl_loss_list = []


start_time = time.time()
for epoch in range(NUM_EPOCHS):
    
    # train
    model.train()

    len_train_dataloader = len(train_dataloader)
    train_data_iter = iter(train_dataloader)

    train_iter = 0
    train_epoch_loss = 0
    train_epoch_recon_loss = 0
    train_epoch_kl_loss = 0
    while train_iter < len_train_dataloader:

        train_data = next(train_data_iter)
        source_seq = train_data
        batch_size = source_seq.shape[0]
        source_seq = source_seq.type(torch.FloatTensor).to(DEVICE)

        # --------------------------
        # Train FeatureExtractor and ClassPredictor
        # --------------------------
        optim.zero_grad()

        recon_seq, z_mean, z_log_var = model(source_seq, source_seq)
        recon_loss = recon_loss_fn(recon_seq, source_seq)
        kl_loss = kl_loss_fn(z_mean, z_log_var)

        loss = recon_loss + kl_loss

        loss.backward()
        optim.step()

        train_epoch_loss += loss.item()
        train_epoch_recon_loss += recon_loss.item()
        train_epoch_kl_loss += kl_loss.item()
        train_iter += 1

    train_epoch_loss_t = train_epoch_loss / len_train_dataloader
    train_epoch_recon_loss_t = train_epoch_recon_loss / len_train_dataloader
    train_epoch_kl_loss_t = train_epoch_kl_loss / len_train_dataloader
    train_epoch_loss_list.append(train_epoch_loss_t)
    train_epoch_recon_loss_list.append(train_epoch_recon_loss_t)
    train_epoch_kl_loss_list.append(train_epoch_kl_loss_t)

    # validate
    model.eval()

    with torch.no_grad():
        len_val_dataloader = len(test_dataloader)
        val_data_iter = iter(test_dataloader)
    
        val_iter = 0
        val_epoch_loss = 0
        val_epoch_recon_loss = 0
        val_epoch_kl_loss = 0
        while val_iter < len_val_dataloader:

            val_data = next(val_data_iter)
            source_seq = val_data
            batch_size = source_seq.shape[0]
            source_seq = source_seq.type(torch.FloatTensor).to(DEVICE)
    
            recon_seq, z_mean, z_log_var = model(source_seq, source_seq)
            recon_loss = recon_loss_fn(recon_seq, source_seq)
            kl_loss = kl_loss_fn(z_mean, z_log_var)
            loss = recon_loss + kl_loss
    
            val_epoch_loss += loss.item()
            val_epoch_recon_loss += recon_loss.item()
            val_epoch_kl_loss += kl_loss.item()
            val_iter += 1
    
        val_epoch_loss_t = val_epoch_loss / len_val_dataloader
        val_epoch_recon_loss_t = val_epoch_recon_loss / len_val_dataloader
        val_epoch_kl_loss_t = val_epoch_kl_loss / len_val_dataloader
        val_epoch_loss_list.append(val_epoch_loss_t)
        val_epoch_recon_loss_list.append(val_epoch_recon_loss_t)
        val_epoch_kl_loss_list.append(val_epoch_kl_loss_t)

    print('Epoch: %03d/%03d | Train/Recon/KL Loss: %.4f/%.4f/%.4f | Val/Recon/KL Loss: %.4f/%.4f/%.4f | Time elapsed: %.2f min'
    %(epoch+1, NUM_EPOCHS, train_epoch_loss_t, train_epoch_recon_loss_t, train_epoch_kl_loss_t, val_epoch_loss_t, val_epoch_recon_loss_t, val_epoch_kl_loss_t, (time.time() - start_time)/60))
```


```
>>> Epoch: 001/1000 | Train/Recon/KL Loss: 0.2880/0.2605/0.0275 | Val/Recon/KL Loss: 0.4382/0.4147/0.0235 | Time elapsed: 0.18 min
    Epoch: 002/1000 | Train/Recon/KL Loss: 0.1395/0.1320/0.0075 | Val/Recon/KL Loss: 0.4244/0.4110/0.0135 | Time elapsed: 0.35 min
    ...
    Epoch: 999/1000 | Train/Recon/KL Loss: 0.0035/0.0035/0.0000 | Val/Recon/KL Loss: 0.1634/0.1634/0.0001 | Time elapsed: 176.45 min
    Epoch: 1000/1000 | Train/Recon/KL Loss: 0.0036/0.0036/0.0000 | Val/Recon/KL Loss: 0.1885/0.1884/0.0001 | Time elapsed: 176.62 min
```

# 추론

학습이 끝나면 임의의 샘플을 재구성해봅니다.   
모델이 어느정도의 규칙과 경향성은 학습한 걸로 보입니다.

![recon_samples](../img/recon_samples.png')

# 결론

처음 Transformer를 스터디할 당시,
성능이 너무 좋은 나머지 NLP를 넘어서 CV 분야(ViT)에서도 쓰이는 것을 보고 "그러면 time-series에서도 통하려나?" 싶은 마음에 공부를 시작했다가,
구현 예제가 모두 NLP와 TransformerEncoder에 집중되어 있고, 다른 예제는 거의 찾을 수 없어 애를 먹었던 기억이 있습니다.   

그래서 time-series에 대한 Transformer 모델 예제를 한번 정리해봤습니다.

Transformer는 token에 강한 특성을 갖기 때문에,
time-series에서도 *Conv1d layer* 등으로 각 time-step을 tokenize해서 학습하곤 하는데, 이런 방식을 참고하면 재구성 성능이 더 올라가지 않을까 합니다.

그리고 본문에서는 하나의 모델을 선언하고 `forward()`에서 Encoder와 Decoder 연산을 같이 구현해주었는데요,
실제 생성 task에서는 Decoder(Generator)에 Gaussian noise를 입력해주어야하기 때문에 Encoder와 Decoder를 따로 구현해주는게 일반적입니다.

마무리를 어떻게 하면 좋을지 모르겠는데, 끝까지 읽어주셔서 감사하고 도움 되셨기를 바랍니다!