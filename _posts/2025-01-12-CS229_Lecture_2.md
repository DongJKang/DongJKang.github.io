---
title: "[CS229] Lecture 2"
date: 2025-01-12 17:00:00 +0900
categories: [Stanford Online]
tags: [gradient descent]     # TAG names should always be lowercase
---

# 들어가기 앞서

본 시리즈의 목적은 CS229 강의 내용을 그대로 담아서 필자의 장기기억 저장소에 업로드하기 위함이므로,
강의 내용을 될 수 있는 한 그대로 따라가려고 합니다.
그래서 강의 흐름에 따라 강의자가 한 대사와 판서 내용을 그대로 적으려고 했습니다.
다만 흐름이 매끄럽지 못하거나 추가 설명이 필요하다 생각되는 부분은 필자가 강의자의 대사를 인용해서,
혹은 필자 나름대로의 생각을 추가하였습니다.
포스팅에서 강의자의 대사와 필자의 생각을 따로 구분하지는 않고 있으니, 이 점 염두에 두시고 읽어주시면 감사하겠습니다.

# y 예측하기

오늘 우리는 y를 예측할 겁니다.
y는 뭐든지 될 수 있습니다.
집값이 될 수도 있고, 주가가 될 수도 있고, 사람들의 나이가 될 수도 있고, 시험 성적이 될 수도 있습니다.
그리고 우리는 *선형회귀(linear regression)* 모델을 사용해서 이것을 예측해보려고 합니다.

*가설(hypothesis)* $h$는 아래와 같이 정의됩니다.
(앞으로 선형회귀 모델은 가설이라는 단어로 부르겠습니다.)

$$
\begin{aligned}
h(x)=&\sum_{j=0}^{n}\theta_{j}x_{j}\\
&where\: x_{0}=1
\end{aligned}
$$

이것을 풀어쓰면,

$$
h(x)=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+...+\theta_{n}x_{n}
$$

$$
\begin{aligned}
\theta&: parameters \\
n&: \#feature \\
m&: \#training\_example \\
(x^i, y^i)&:i^{th}\,training\_example
\end{aligned}
$$

$$
\theta=\begin{bmatrix}
\theta_{0}\\ 
\theta_{1}\\
\theta_{2}\\
...\\
\theta_{n}
\end{bmatrix} 
\ \ \ \ 
x=\begin{bmatrix}
x_{0}\\ 
x_{1}\\
x_{2}\\
...\\
x_{n}
\end{bmatrix}
$$


여기서 $\#$은 '~의 개수'를 의미합니다.
그리고 $\theta$와 $x$의 차원은 $n+1 (\#feature+\#bias)$이 됩니다.


# $\theta$ 계산하기

우리의 목표는 $\theta$를 구하는 것입니다.

$$
\theta \ s.t. \ h(x)\approx y \ for \ training\_example
$$

여기서 $s.t.$란 such that의 약자로 '~를 만족하는'으로 번역됩니다.
가령 'A s.t. B'는 'B를 만족하는 A'를 뜻합니다.

그래서 위 수식은 '$\theta$는 모든 학습 데이터 $x$에 대해서 우리의 가설 $h$가 내놓은 예측 $h(x)$가
진짜 정답 $y$와 근사($\approx$)하도록 하는 값이다' 라는 말이 됩니다.

이것을 흔히 아래처럼 표기하기도 하는데요

$$
h_{\theta}(x)
$$

이것은 가설 $h$가 $\theta$와 $x$ 모두에 의해서 결정된다는 것을 강조하기 위한 표현입니다.
즉 $h$는 입력으로 $x$를 받고 $\theta$에 의해 조작(parameterize)되는 함수입니다.

$h(x)\approx y$인지 아닌지를 계산하기 위해서, 우리는 손실함수 $J(\theta)$가 필요합니다.

$$
J(\theta)=\min_{\theta}\frac{1}{2}\sum_{i=1}^{m}(h(x^i)-y^i)^2
$$

위 수식에서 상수 $\frac{1}{2}$은 오로지 계산을 단순화시키기 위해 존재하는 항입니다.
그리고 우리는 MAE 대신 MSE를 사용합니다. 그 이유에 대해서는 다음 주차에 논의합니다.

그렇다면 어떻게 $J(\theta)$를 찾을 수 있을까요?

바로 *gradient descent*를 사용하면 됩니다.


제일 처음은 $\theta$를 아무 값으로 정의하는 것으로 시작합니다.
보통은 0벡터로 시작합니다.

$$
\theta = \vec{0}
$$

그리고나서 $J(\theta)$ 값을 작아지게 하기 위해 $\theta$값을 계속 바꿔줄겁니다.

$\theta$값을 어떻게 바꿔줄 건지는 아래의 수식으로 정의할 수 있습니다.

$$
\theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

여기서 $:=$는 할당(assignment)를 의미합니다. 마치 코딩에서 a := a + 1이 성립하는 것과 같다고 보면 됩니다.
(만약 모든 특징 $x$가 $(-1,1)$의 범위를 갖도록 스케일링되었다면, 경험적으로 $\alpha=0.01$가 좋습니다.)

이제 위의 수식을 계산해봅시다.
계산의 편의를 위해 $m=1$로 가정합니다.


$$
\begin{align*} 
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\\
&=2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(\theta_0+\theta_1x_1+\theta_2x_2+...)\\
&=(h_\theta(x)-y)\cdot x_y
\end{align*} 
$$

결과적으로,

$$
\theta_j := \theta_j - \alpha\sum^{m}_{i=1}(h_\theta(x^i)-y^i)\cdot x^i_j
$$


# Gradiend descent의 종류

Gradient descent에는 두 가지 종류가 있습니다.

1. Batch gradient descent  
데이터셋의 전체 batch를 본 뒤, paramteter update를 한번 수행합니다. 한번의 update만으로도 global minimum에 도달할 수 있지만, 시간이 너무 오래걸리고, 만약 데이터셋이 매우 크다면 계산 자체가 불가능합니다.
2. Stochastic gradient descent  
Batch에서 훈련 샘플을 랜덤(stochastic)하게 뽑아 매 샘플마다 parameter를 update합니다. 따라서 계산이 비교적 빠릅니다. 각각의 훈련 샘플은 서로 다르기 때문에, 단 하나의 global minimum으로 converge되지는 않습니다. 하지만 충분히 큰 숫자의 데이터셋이 있다면, 결국에는 global minimum 가까이에 도달합니다.

결과적으로 만약 데이터셋이 정말 정말 크다면, 현실적으로 batch gradient descent는 사용하지 않습니다. 대신 stochastic gradient descent를 사용하고 학습이 진행됨에 따라 $\alpha$를 감소시킵니다.

# 선형대수를 활용한 gradient descent

단 한번의 계산만으로 global minimum을 찾을 수 있는 방법이 있습니다.

그것은 바로 *matrix derivate*.

다시 우리의 문제로 돌아가봅시다.
우리가 하고 싶은 것은 $J(\theta)$를 최소화하는 $\theta$를 찾는 것입니다.
그래서 만약 우리가 $J(\theta)$를 미분할 수 있다면, 우리는 $J(\theta)$의 미분이 0인 지점을 찾을 수 있을 것이고, 그렇다면 그 지점이 바로 우리가 찾는 $\theta$가 될 것입니다.
$$
\nabla_\theta J(\theta)=\vec{0}
$$
왜냐하면 최소값이나 최대값은 항상 기울기가 0인 지점에 위치하기 때문입니다.

예를 들어,
$$
A=
\begin{bmatrix}
A_{11} & A_{12}\\
A_{21} & A_{22}
\end{bmatrix}
$$
이고,
$$
f(A)=A_{11}+A_{12}^2
$$
이라면,  
$f(A)$의 미분은 다음과 같습니다.

$$
\nabla_A f(A)=
\begin{bmatrix}
\frac{\partial f}{\partial A_{11}} & \frac{\partial f}{\partial A_{12}}\\
\frac{\partial f}{\partial A_{21}} & \frac{\partial f}{\partial A_{22}}
\end{bmatrix}
=
\begin{bmatrix}
1 & 2A_{12}\\
0 & 0
\end{bmatrix}
$$

즉, 단 하나의 수식만으로 끝이 납니다.  
그래서, 이제 하나의 질문만이 남습니다.

Q: $\nabla_\theta J(\theta)=\vec{0}$를 쉽게 찾을 수 있는 방법은 없을까요?  
A: 있습니다. 바로 선형대수를 사용하면 됩니다.

우선 입력 $X$와 라벨 $Y$, 그리고 선형회귀 모델 파라미터 $\theta$를 정의합니다.

$$
X=
\begin{bmatrix}
-x^{(1)T}-\\
-x^{(2)T}-\\
...\\
-x^{(m)T}-
\end{bmatrix}
\ \ \ \ \ \ 
Y=
\begin{bmatrix}
y^{(1)}\\
y^{(2)}\\
...\\
y^{(m)}
\end{bmatrix}
\ \ \ \ \ \ 
\theta=
\begin{bmatrix}
\theta_0\\
\theta_1\\
\theta_2
\end{bmatrix}
$$

그리고 손실함수 $J(\theta)$를 유도합니다.

$$
X\theta=
\begin{bmatrix}
x^{(1)T}\theta\\
x^{(2)T}\theta\\
...\\
x^{(m)T}\theta
\end{bmatrix}
=
\begin{bmatrix}
h_\theta(x^{(1)})\\
h_\theta(x^{(2)})\\
...\\
h_\theta(x^{(m)})
\end{bmatrix}
$$

$$
X\theta-Y=
\begin{bmatrix}
h_\theta(x^{(1)})-y^{(1)}\\
h_\theta(x^{(2)})-y^{(2)}\\
...\\
h_\theta(x^{(m)})-y^{(m)}
\end{bmatrix}
$$

$$
\begin{align*}
J(\theta)&=\frac{1}{2}\sum^{m}_{i=1}(h_\theta x^{(i)}-y^{(i)})^2\\
&=\frac{1}{2}(X\theta-Y)^T(X\theta-Y)
\end{align*}
$$

이제 $J_\theta$ 미분을 행렬 버전으로 구해봅시다.

$$
\begin{align*}
\nabla_\theta J(\theta)&=\frac{1}{2}\nabla_\theta(X\theta-Y)^T(X\theta-Y)\\
&=\frac{1}{2}\nabla_\theta(\theta^TX^T-Y^T)(X\theta-Y)\\
&=\frac{1}{2}\nabla_\theta\left[\theta^TX^TX\theta-\theta^TX^TY-Y^TX\theta+Y^TY\right]\\
&=\frac{1}{2}\left[X^TX\theta+X^TX\theta-X^TY-X^TY\right]\\
&=X^TX\theta-X^TY\\
&=\vec{0}
\end{align*}
$$

결과적으로 아래의 *normal equation*을 얻을 수 있습니다.

$$
X^TX\theta=X^TY
$$

여기서 $X^TX$를 우항으로 넘기면, 우리가 원하는 $\theta$를 단 하나의 수식으로 표현할 수 있습니다.

$$
\theta=(X^TX)^{-1}X^TY
$$